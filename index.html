<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=gb2312" />
<title>Zhaozhuo Xu</title>
</head>


<span class="pull-left"><img src="zhaozhuo_pic.jpg" height="150" width="160"></span>
<body>
<p><span style="font-weight: bold;">Zhaozhuo Xu</span></p>
<p>Email: zxu79 at stevens dot edu</p>
<a href="https://scholar.google.com/citations?hl=en&user=7tDlVAsAAAAJ&view_op=list_works&sortby=pubdate">[Scholar]</a><a href="https://github.com/Ottovonxu">[Github]</a><a href="https://www.stevens.edu/profile/zxu79">[Stevens Profile]</a>

<ul>
I am starting as an Assistant Professor of Computer Science at Stevens Institute of Technology in Spring 2024.
I received my Ph.D. in Computer Science from Rice University under the supervision of professor <a href="https://www.cs.rice.edu/~as143/">Anshumali Shrivastava</a>. My research focuses on scaling up machine learning on commodity hardware using randomized algorithms.
</ul>
<!-- <ul>
I am organizing the Research On Algorithms & Data Structures (ROADS) to Mega-AI Models Workshop at MLSys! <a href="https://roads2megaai.github.io/">Link</a> KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache
</ul> -->
</div>
<p><span style="font-weight: bold;">Selected Publication</span></p>
<em>Journal</em>
<p>&dagger; indicates corresponding authors</p>
<ul>
<li>
Yuheng Wu, Wentao Guo, Zirui Liu, Heng Ji, <strong>Zhaozhuo Xu</strong><sup>&dagger<\sup>; & Denghui Zhang<sup>&dagger<\sup>, &quot;How large language models encode theory-of-mind: a study on sparse parameter patterns", <b>npj Artificial Intelligence</b> 2025. <a href="https://www.nature.com/articles/s44387-025-00031-9">PDF</a> <a href="https://github.com/joel-wu/how-large-language-models-encode-theory-of-mind">Code</a></li>  
</ul>



<em>Conference</em>
<p>* indicates equal contribution</p>


<ul>
<li>
Zhuang Wang*, <strong>Zhaozhuo Xu</strong>*, Jingyi Xi*, Yuke Wang, Anshumali Shrivastava and T. S. Eugene Ng, &quot;ZEN: Empowering Distributed Training with Sparsity-driven Data Synchronization", <b>OSDI</b> 2025. <a href="https://www.usenix.org/system/files/osdi25-wang-zhuang.pdf">PDF</a> <a href="https://github.com/zhuangwang93/ZEN">Code</a></li>  
</ul>

<ul>
<li>
Tianyi Zhang, Junda Su, Aditya Desai, Oscar Wu, <strong>Zhaozhuo Xu</strong> and Anshumali Shrivastava, &quot;Sketch to Adapt: Fine-Tunable Sketches for Efficient LLM Adaptation", <b>ICML</b> 2025. <a href="https://arxiv.org/abs/2410.06364">PDF</a> </li>  
</ul>

<ul>
<li>
Wentao Guo, Jikai Long, Yimeng Zeng, Zirui Liu, Xinyu Yang, Yide Ran, Jacob R. Gardner, Osbert Bastani, Christopher De Sa, Xiaodong Yu, Beidi Chen and <strong>Zhaozhuo Xu</strong>, &quot;Zeroth-Order Fine-Tuning of LLMs with Transferable Static Sparsity", <b>ICLR</b> 2025. <a href="https://openreview.net/pdf?id=myYzr50xBh">PDF</a></li> 
</ul>

<ul>
<li>
Yanzhou Pan, Huawei Lin, Yide Ran, Jiamin Chen, Xiaodong Yu, Weijie Zhao, Denghui Zhang and <strong>Zhaozhuo Xu</strong>, &quot;ALinFiK: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Party LLM Data Valuation", <b>NAACL</b> 2025 <strong>Oral</strong>. <a href="https://arxiv.org/abs/2503.01052">PDF</a></li> 
</ul>


<ul>
<li>
Tianyi Zhang, Jonah Yi, <strong>Zhaozhuo Xu</strong> and Anshumali Shrivastava, &quot;KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization", <b>NeurIPS</b> 2024. <a href="https://arxiv.org/abs/2405.03917">PDF</a></li> 
</ul>

<ul>
<li>
Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, <strong>Zhaozhuo Xu</strong> and Anshumali Shrivastava, &quot;NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention", <b>NeurIPS</b> 2024. <a href="https://arxiv.org/abs/2403.01273">PDF</a> <a href="https://github.com/tonyzhang617/nomad-dist">Code</a></li> 
</ul>


<ul>
<li>
Yang Zhou, Zhuoming Chen, <strong>Zhaozhuo Xu</strong>, Victoria Lin and Beidi Chen, &quot;Sirius: Contextual Sparsity with Correction for Efficient LLMs", <b>NeurIPS</b> 2024. <a href="https://www.arxiv.org/abs/2409.03856">PDF</a> <a href="https://github.com/Infini-AI-Lab/Sirius">Code</a></li> 
</ul>


<ul>
<li>
Jialiang Xu, Shenglan Li, <strong>Zhaozhuo Xu</strong> and Denghui Zhang, &quot;Do LLMs Know to Respect Copyright Notice?", <b>EMNLP</b> 2024. <a href="https://aclanthology.org/2024.emnlp-main.1147/">PDF</a> <a href="https://github.com/liamjxu/LLM-copyright">Code</a></li> 
</ul>

<ul>
  <li>
    Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, <strong>Zhaozhuo Xu</strong>, Zirui Liu and Xia Hu, "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", Findings of <b>EMNLP</b> 2024. <a href="https://aclanthology.org/2024.findings-emnlp.266/">PDF</a>  <a href="https://github.com/henryzhongsc/longctx_bench">Code</a>
  </li>
</ul>
<ul>
  <li>
    Junda Su, Zirui Liu, Zeju Qiu, Weiyang Liu and <strong>Zhaozhuo Xu</strong>, "In Defense of Structural Sparse Adapters for Concurrent LLM Serving", Findings of <b>EMNLP</b> 2024. <a href="https://aclanthology.org/2024.findings-emnlp.284/">PDF</a>
  </li>
</ul>


<ul>
<li>
Huawei Lin, Jikai Long, <strong>Zhaozhuo Xu</strong> and Weijie Zhao, &quot;Token-wise Influential Training Data Retrieval for Large Language Models", <b>ACL</b> 2024. <a href="https://aclanthology.org/2024.acl-long.48/">PDF</a> <a href="https://github.com/huawei-lin/RapidIn">Code</a></li> 
</ul>
<ul>
<li>
Duy Le, Shaochen Zhong, Zirui Liu, Shuai Xu, Vipin Chaudhary, Kaixiong Zhou and <strong>Zhaozhuo Xu</strong>, &quot;Knowledge Graphs Can be Learned with Just Intersection Features", <b>ICML</b> 2024. <a href="https://proceedings.mlr.press/v235/le24c.html">PDF</a></li>
</ul>
<ul>
<li>
<strong>Zhaozhuo Xu</strong>*, Zirui Liu*, Beidi Chen, Shaochen (Henry) Zhong, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu and Anshumali Shrivastava, &quot;Soft Prompt Recovers Compressed LLMs, Transferably", <b>ICML</b> 2024. <a href="https://proceedings.mlr.press/v235/xu24s.html">PDF</a> <a href="https://github.com/zirui-ray-liu/compress-then-prompt">Code</a></li> 
</ul>
<ul>
<li>
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen (Henry) Zhong, <strong>Zhaozhuo Xu</strong>, Vladimir Braverman, Beidi Chen, and Xia Hu, &quot;KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache", <b>ICML</b> 2024. <a href="https://proceedings.mlr.press/v235/liu24bz.html">PDF</a> <a href="https://github.com/jy-yuan/KIVI">Code</a></li>
</ul>
<ul>
<li>
Guanchu Wang, Yu-Neng Chuang, Fan Yang, Mengnan Du, Chia-Yuan Chang, Shaochen Zhong, Zirui Liu, <strong>Zhaozhuo Xu</strong>, Kaixiong Zhou, Xuanting Cai and Xia Hu, &quot;TVE: Learning Meta-attribution for Transferable Vision Explainer", <b>ICML</b> 2024. <a href="https://proceedings.mlr.press/v235/wang24j.html">PDF</a> <a href="https://github.com/guanchuwang/TVE">Code</a></li>
</ul>
<ul>
<li>
Shaochen Zhong, Duy Le, Zirui Liu, Zhimeng Jiang, Andrew Ye, Jiamu Zhang, Jiayi Yuan, Kaixiong Zhou, <strong>Zhaozhuo Xu</strong>, Jing Ma, Shuai Xu, Vipin Chaudhary and Xia Hu, &quot;GNNs Also Deserve Editing, and They Need It More Than Once", <b>ICML</b> 2024. <a href="https://proceedings.mlr.press/v235/zhong24d.html">PDF</a> <a href="https://github.com/henryzhongsc/gnn_editing">Code</a></li>
</ul>
<ul>
<li>
Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, <strong>Zhaozhuo Xu</strong>, Anastasios Kyrillidis and Anshumali Shrivastava, &quot;Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time", <b>NeurIPS</b> 2023. <a href="https://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html">PDF</a></li> 
</ul>
<ul>
<li>
Zichang Liu*, <strong>Zhaozhuo Xu</strong>*, Benjamin Coleman and Anshumali Shrivastava, &quot;One-Pass Distribution Sketch for Measuring Data Heterogeneity in Federated Learning", <b>NeurIPS</b> 2023. <a href="https://papers.nips.cc/paper_files/paper/2023/hash/32c2f3e0a44d55820da7fbcee0a1d95c-Abstract-Conference.html">PDF</a></li> 
</ul>
<ul>
<li>
Zirui Liu*, Guanchu Wang*, Shaochen Zhong, <strong>Zhaozhuo Xu</strong>, Daochen Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu and Xia Hu, &quot;Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model", <b>NeurIPS</b> 2023. <a href="https://papers.nips.cc/paper_files/paper/2023/hash/0a6059857ae5c82ea9726ee9282a7145-Abstract-Conference.html">PDF</a></li> 
</ul>
<ul>
<li>
Tianyi Zhang, Zhenwei Dai, <strong>Zhaozhuo Xu</strong> and Anshumali Shrivastava, &quot;Graph Self-supervised Learning via Proximity Divergence Minimization", <b>UAI</b> 2023. <a href="https://proceedings.mlr.press/v216/zhang23d/zhang23d.pdf">PDF</a> <a href="https://github.com/tonyzhang617/pdm">Code</a></li> 
</ul>
<ul>
<li>
Zhuang Wang, Xinyu Wu, <strong>Zhaozhuo Xu</strong> and T. S. Eugene Ng, &quot;Cupcake: A Compression Scheduler for Scalable Communication-Efficient Distributed Training", <b>MLSys</b> 2023. <a href="https://proceedings.mlsys.org/paper_files/paper/2023/file/870ac3ae9e2d5a29fc41e6ad645ded8f-Paper-mlsys2023.pdf">PDF</a></li>
</ul>
<ul>
<li>
<strong>Zhaozhuo Xu</strong>, Zhao Song and Anshumali Shrivastava, &quot;A Tale of Two Efficient Value Iteration Algorithms for Solving Linear MDPs with Large Action Space", <b>AISTATS</b> 2023. <a href="https://proceedings.mlr.press/v206/xu23b.html">PDF</a></li>
</ul>
<ul>
<li>
Tianyi Zhang*, <strong>Zhaozhuo Xu</strong>*, Tharun Medini and Anshumali Shrivastava, &quot;Structural Contrastive Representation Learning for Zero-shot Multi-label Text Classification", <b>EMNLP </b> Findings 2022. <a href="https://aclanthology.org/2022.findings-emnlp.362.pdf">PDF</a> <a href="https://github.com/tonyzhang617/structural-contrastive-representation-learning">Code</a></li>
</ul>
<ul>
<li>
Zhuang Wang*, <strong>Zhaozhuo Xu</strong>*, Xinyu Crystal Wu, Anshumali Shrivastava and T. S. Eugene Ng, &quot;DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks", <b>ICML </b> 2022. <a href="https://proceedings.mlr.press/v162/wang22aj/wang22aj.pdf">PDF</a></li>
</ul>
<ul>
<li>
Zichang Liu*, <strong>Zhaozhuo Xu</strong>*, Alan Ji, Junyan Zhang, Jonathan Li, Beidi Chen and Anshumali Shrivastava, &quot;HALOS: Hashing Large Output Space for Cheap Inference", <b>MLSys </b> 2022. <a href="https://proceedings.mlsys.org/paper/2022/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html">PDF</a></li>
</ul>
<ul>
<li>
<strong>Zhaozhuo Xu</strong>, Zhao Song and Anshumali Shrivastava, &quot;Breaking the Linear Iteration Cost Barrier for Some Well-known Conditional Gradient Methods Using MaxIP Data-structures", <b>NeurIPS </b> 2021. <a href="https://proceedings.neurips.cc/paper/2021/hash/2c27a260f16ad3098393cc529f391f4a-Abstract.html">PDF</a></li>
</ul>
<ul>
<li>
<strong>Zhaozhuo Xu</strong>, Beidi Chen, Chaojian Li, Weiyang Liu, Le Song, Yingyan Lin and Anshumali Shrivastava, &quot;Locality Sensitive Teaching", <b>NeurIPS </b> 2021. <a href="https://proceedings.neurips.cc/paper/2021/hash/95c3f1a8b262ec7a929a8739e21142d7-Abstract.html">PDF</a> <a href="https://github.com/Ottovonxu/LST">Code</a></li>
</ul>
<ul>
<li>
Aditya Desai*, <strong>Zhaozhuo Xu</strong>*, Menal Gupta, Anu Chandran, Antoine Vial-Aussavy and Anshumali Shrivastava, &quot;Raw Nav-merge Seismic Data to Subsurface Properties with MLP based Multi-Modal Information Unscrambler", <b>NeurIPS </b> 2021. <a href="https://papers.nips.cc/paper/2021/hash/498f2c21688f6451d9f5fd09d53edda7-Abstract.html">PDF</a></li>
</ul>
<ul>
<li>
Shulong Tan, <strong>Zhaozhuo Xu</strong>, Weijie Zhao,  Hongliang Fei, Zhixin Zhou and Ping Li, &quot;Norm Adjusted Proximity Graph for Fast Inner Product Retrieval", <b>KDD </b> 2021. <a href="https://dl.acm.org/doi/10.1145/3447548.3467412">PDF</a></li>
</ul>
<ul>
<li>
Beidi Chen, Zichang Liu, Binghui Peng, <strong>Zhaozhuo Xu</strong>, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava and Christopher Re, &quot;MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training ", <b>ICLR </b> 2021 <strong>Oral</strong>. <a href="https://openreview.net/forum?id=wWK7yXkULyh">PDF</a> <a href="https://github.com/HazyResearch/mongoose">Code</a></li>
</ul>
<ul>
<li>Shulong Tan, Zhixin Zhou, <strong>Zhaozhuo Xu</strong> and Ping Li, &quot;Fast Item Ranking under Neural Network based Measures", <b>WSDM </b> 2020. <a href="https://dl.acm.org/doi/pdf/10.1145/3336191.3371830">PDF</a></li>
</ul>
<ul>
<li>
<strong>Zhaozhuo Xu</strong>, Alan Baonan Ji, Andrew Woods, Beidi Chen and Anshumali Shrivastava, &quot;Satellite Images and Deep Learning to Identify Discrepancy in Mailing Addresses with Applications to Census 2020 in Houston", <b>JSM </b> 2020. <a href="https://arxiv.org/pdf/2111.06562.pdf">PDF</a></li>
</ul>
<ul>
<li>Zhixin Zhou, Shulong Tan, <strong>Zhaozhuo Xu</strong> and Ping Li, &quot;M&oumlbius Transformation for Fast Inner Product Search on Graph", <b>NeurIPS </b> 2019. <a href="https://papers.nips.cc/paper/9032-mobius-transformation-for-fast-inner-product-search-on-graph.pdf">PDF</a> <a href="https://github.com/sunbelbd/mobius">Code</a></li>
</ul>
<ul>
<li>Shulong Tan, Zhixin Zhou, <strong>Zhaozhuo Xu</strong> and Ping Li, &quot;On Efficient Retrieval of Top Similarity Vectors", <b>EMNLP</b> 2019. <a href="https://www.aclweb.org/anthology/D19-1527.pdf">PDF</a></li>
</ul>



<!-- <p><span style="font-weight: bold;">Presentation</span></p>
<ul>
<li>&quot;DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks", <b> Oral Spotlight Presentation in Sparsity in Neural Networks Workshop </b> 2022. <a href="https://proceedings.mlr.press/v162/wang22aj/wang22aj.pdf">Report</a></li>
</ul>
<ul>
<li>&quot;Beyond Convolutions: A Novel Deep Learning Approach for Raw Seismic Data Ingestion", <b> Energy Polymer Group Spring Technical Meeting </b> 2022. <a href="https://arxiv.org/pdf/2102.13631.pdf">Report</a></li>
</ul>
<ul>
<li>&quot;PISTACHIO: Patch Importance Sampling To Accelerate CNNs via a Hash Index Optimizer", <b>Medical Imaging meets NeurIPS (MedNeurIPS) </b> 2021. <a href="http://www.cse.cuhk.edu.hk/~qdou/public/medneurips2021/60_PISTACHIO.pdf">Report</a></li>
</ul>
<ul>
<li>&quot;Satellite Images and Deep Learning to Identify Discrepancy in Mailing Addresses with Applications to Census 2020 in Houston", <b>Joint Statistical Meetings (JSM) </b> 2020. <a href="https://arxiv.org/abs/2111.06562">Report</a></li>
</ul> -->


<p><span style="font-weight: bold;">Tutorial</span></p>
<ul>
<li><strong>AAAI 2025:</strong> KV Cache Compression for Efficient Long Context LLM Inference: Challenges, Trade-Offs, and Opportunities. <a href="https://aaai.org/conference/aaai/aaai-25/tutorial-and-lab-list/#TQ08">Link</a></li>
</ul>
<ul>
<li><strong>AAAI & NAACL 2025:</strong> LLMs and Copyright Risks: Benchmarks and Mitigation Approaches. <a href="https://aclanthology.org/2025.naacl-tutorial.7/">Link</a></li>
</ul>

<p><span style="font-weight: bold;">Honor</span></p>
<ul>
<li>AAAI-25 New Faculty Highlights. <a href="https://aaai.org/conference/aaai/aaai-25/new-faculty-highlights-program/">Link</a></li>
</ul>
<ul>
<li>Ken Kennedy Institute BP Fellowship, 2021 - 2022. <a href="https://kenkennedy.rice.edu/news/current-news/ken-kennedy-institute-graduate-fellowship-program-awards-65000">Link</a></li>
</ul>




<p><span style="font-weight: bold;">Service</span></p>
<ul>
<li><strong>Organizer:</strong> The VISTA: Visionary Innovation in Standards and Technology of GenAI workshop at ICDM 2025. <a href="https://vista-genai.github.io/">Link</a></li>
</ul>
<ul>
<li><strong>Organizer:</strong> Research On Algorithms & Data Structures (ROADS) to Mega-AI Models Workshop, MLSys 2023. <a href="https://roads2megaai.github.io/">Link</a></li>
</ul>
<ul>
<li><strong>Area Chair:</strong> NeurIPS, ICLR, ICML, ARR, EMNLP Main/Demo Track, NAACL, COLING</li>
</ul>



